---
title: "HW2 - News Analytics"
output: html_notebook
---
## Ria Gandhi

## Part 1

**Importing Data**
```{r, show = FALSE}
library(readr)
archive = read_csv("~/Downloads/OIDD 245/NewsArticles.csv")
```

**Transforming Data**
I used the stopwords-iso dictionary for my stopwords as that gave me more accurate words to group into topics.
```{r, include = TRUE}

library(tm)
library(stopwords)
library(topicmodels)
library(tidytext)
library(dplyr)
library(ggplot2)

#Content column into corpus
input = archive$content
vec = VectorSource(input)
corp = VCorpus(vec) 

#Cleaning content
corp2 = tm_map(corp, removePunctuation)
corp2 = tm_map(corp2, removeNumbers)
corp2 = tm_map(corp2, content_transformer(removeWords), stopwords(language = "en", source = "stopwords-iso"), lazy = TRUE)
corp2 = tm_map(corp2, content_transformer(tolower), lazy = TRUE)
corp2 = tm_map(corp2, stripWhitespace)

#Generate Document-term Matrix
dtm = DocumentTermMatrix(corp2)
dtms = removeSparseTerms(dtm, .985)
m2 = as.matrix(dtms)

#LDA Model
train = m2[1:7000,]
train = na.omit(train)
lda_model = LDA(train, 15, method = "Gibbs")

# Finding Betas
topics <- tidy(lda_model, matrix = "beta")

top_terms <- 
  topics %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)

top_terms %>%
  mutate(term = reorder(term, beta)) %>%
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  coord_flip()
```

**Determining Topics**
I used the Gibbs method as that gave me the best results, applying the LDA model to 7000 articles. In order to determine the best word groupings, I generated this table of graphs showing the betas for each set of word groupings.

```{r, include = TRUE}

top_terms %>%
  mutate(term = reorder(term, beta)) %>%
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  coord_flip()


```


After looking at the top 20 words for each of the 15 topics generated by the LDA model, I named each topic. However, some of the topics were redundant and/or I didn't think that the words were related enough to the topic. Therefore, I removed World and Economy2, to end up with a total of 13 topics.

```{r}
terms = terms(lda_model, 20)
colnames(terms) = c("Energy", "Retail", "Economy2", "Markets", "Government", "Business", "Auto", "Personal Finance", "New York", "Healthcare", "World", "Sports", "Economy", "Finance", "Technology")
terms = as.data.frame(terms)
terms$World = NULL
terms$Economy2 = NULL
terms
```

## Part 2

**Scraping CNBC and Cleaning Article Text**

I scraped the first 3 pages on CNBC to get a total of 56 articles and then cleaned it.
```{r}
library(rvest)
library(magrittr)

cnbc = as.data.frame(NULL)

for (i in 1:3) {
  urls = read_html(paste("https://www.cnbc.com/us-news/?page=",i, sep="")) 
  links = urls %>% html_nodes(".desc_size160_105.card .headline a") %>% html_attr("href")
  titles = urls %>% html_nodes(".desc_size160_105.card .headline a") %>% html_text()
  
  s = data.frame(titles = titles, urls = links, stringsAsFactors = FALSE)
  cnbc = rbind(cnbc,s)
}
  cleaned_articles = as.data.frame(NULL)

for (j in 1:nrow(cnbc)) {
  txt = read_html(paste("https://www.cnbc.com", cnbc$urls[j], sep="")) %>% html_nodes(".group p") %>%  html_text()
  txt = paste(txt, sep = " ", collapse = " ") 
  txt %>% gsub("\n", "", .) %>% gsub('\r', "", .) %>% gsub('\t', "", .) %>% gsub('\\"', '', .,fixed = TRUE)

  s2 = data.frame(text = txt, stringsAsFactors = FALSE)
  cleaned_articles = rbind(cleaned_articles, s2)
}
```

```{r}
# This is an example of a cleaned article
head(cleaned_articles, 1)
```


```{r}
#Content column into corpus
i = cleaned_articles$text
v = VectorSource(i)
corp_f = VCorpus(v) 

# Specify this dictionary when creating the dtm for the new articles, which will limit the dtm it creates to only the words that also appeared in the archive. 

dic = Terms(dtms)

new_dtm = DocumentTermMatrix(corp_f, control=list(dictionary = dic))
new_dtm = new_dtm[rowSums(as.matrix(new_dtm))!=0,]
topic_probabilities = posterior(lda_model, new_dtm)

#Probability Matrix
probabilities = as.data.frame(topic_probabilities$topics)
probabilities$'3' = NULL
probabilities$'11' = NULL
colnames(probabilities) = c("Energy", "Retail", "Markets", "Government", "Business", "Auto", "Personal Finance", "New York", "Healthcare", "Sports", "Economy", "Finance", "Technology")

#Find the topic for each article
probabilities$max = colnames(probabilities)[max.col(probabilities, ties.method="first")]
cleaned_articles = cbind(cleaned_articles, probabilities$max)

final = bind_rows(list(cleaned_articles[1,], cleaned_articles[3:5,], cleaned_articles[10,],
                       cleaned_articles[12,], cleaned_articles[15,], cleaned_articles[17,],
                       cleaned_articles[18,], cleaned_articles[21,]))
final$text = substr(final$text, start = 1, stop = 80)
```

```{r}
final
```

